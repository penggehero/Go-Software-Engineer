# 第1章　ClickHouse的前世今生

Google于2003～2006年相继发表了三篇论文“Google FileSystem”“Google MapReduce”和“Google Bigtable”，将大数据的处理技术带进了大众视野。2006年开源项目Hadoop的出现，标志着大数据技术普及的开始，大数据技术真正开始走向普罗大众。长期以来受限于数据库处理能力而苦不堪言的各路豪杰们，仿佛发现了新大陆，于是一轮波澜壮阔的技术革新浪潮席卷而来。Hadoop最初指代的是分布式文件系统HDFS和MapReduce计算框架，但是它一路高歌猛进，在此基础之上像搭积木一般快速发展成为一个庞大的生态（包括Yarn、Hive、HBase、Spark等数十种之多）。在大量数据分析场景的解决方案中，传统关系型数据库很快就被Hadoop生态所取代，我所处的BI领域就是其中之一。传统关系型数据库所构建的数据仓库，被以Hive为代表的大数据技术所取代，数据查询分析的手段也层出不穷，Spark、Impala、Kylin等百花齐放。Hadoop发展至今，早已上升成为大数据的代名词，仿佛一提到海量数据分析场景下的技术选型，就非Hadoop生态莫属。

然而世间并没有银弹（万全之策），Hadoop也跳不出这个规则。虽然Hadoop生态化的属性带来了诸多便利性，例如分布式文件系统HDFS可以直接作为其他组件的底层存储（例如HBase、Hive等），生态内部的组件之间不用重复造轮子，只需相互借力、组合就能形成新的方案。但生态化的另一面则可以看作臃肿和复杂。Hadoop生态下的每种组件都自成一体、相互独立，这种强强组合的技术组件有些时候显得过于笨重了。与此同时，随着现代化终端系统对实效性的要求越来越高，Hadoop在海量数据和高时效性的双重压力下，也显得有些力不从心了。

次机缘巧合，在研究BI产品技术选型的时候，我接触到了ClickHouse，瞬间就被其惊人的性能所折服。这款非Hadoop生态、简单、自成一体的技术组件引起了我极大的好奇。那么ClickHouse究竟是什么呢？下面就让我们一探究竟，聊一聊ClickHouse的前世今生。

## 1.1 传统BI系统之殇

得益于IT技术的迅猛发展，ERP、CRM这类IT系统在电力、金融等多个行业均得以实施。这些系统提供了协助企业完成日常流程办公的功能，其应用可以看作线下工作线上化的过程，这也是IT时代的主要特征之一，通常我们把这类系统称为联机事务处理（OLTP）系统。企业在生产经营的过程中，并不是只关注诸如流程审批、数据录入和填报这类工作。站在监管和决策层面，还需要另一种分析类视角，例如分析报表、分析决策等。而IT系统在早期的建设过程中多呈烟囱式发展，数据散落在各个独立的系统之内，相互割裂、互不相通。

为了解决数据孤岛的问题，人们提出了数据仓库的概念。即通过引入一个专门用于分析类场景的数据库，将分散的数据统一汇聚到一处。借助数据仓库的概念，用户第一次拥有了站在企业全局鸟瞰一切数据的视角。

随着这个概念被进一步完善，一类统一面向数据仓库，专注于提供数据分析、决策类功能的系统与解决方案应运而生。最终于20世纪90年代，有人第一次提出了BI（商业智能）系统的概念。自此以后，人们通常用BI一词指代这类分析系统。相对于联机事务处理系统，我们把这类BI系统称为联机分析（OLAP）系统。

传统BI系统的设计初衷虽然很好，但实际的应用效果却不能完全令人满意。我想之所以会这样，至少有这么几个原因。

首先，传统BI系统对企业的信息化水平要求较高。按照传统BI系统的设计思路，通常只有中大型企业才有能力实施。因为它的定位是站在企业视角通盘分析并辅助决策的，所以如果企业的信息化水平不高，基础设施不完善，想要实施BI系统根本无从谈起。这已然把许多潜在用户挡在了门外。

其次，狭小的受众制约了传统BI系统发展的生命力。传统BI系统的主要受众是企业中的管理层或决策层。这类用户虽然通常具有较高的话语权和决策权，但用户基数相对较小。同时他们对系统的参与度和使用度不高，久而久之这类所谓的BI系统就沦为了领导视察、演示的“特供系统”了。

最后，冗长的研发过程滞后了需求的响应时效。传统BI系统需要大量IT人员的参与，用户的任何想法，哪怕小到只是想把一张用饼图展示的页面换成柱状图展示，可能都需要依靠IT人员来实现。一个分析需求从由用户大脑中产生到最终实现，可能需要几周甚至几个月的时间。这种严重的滞后性仿佛将人类带回到了飞鸽传书的古代。

## 1.2 现代BI系统的新思潮

技术普惠是科技进步与社会发展的一个显著特征。从FM频段到GPS定位乃至因特网都遵循着如此的发展规律。有时我们很难想象，这些在现今社会习以为常的技术，其实在几十年前还仅限于服务军队这类少数群体。

每一次技术普惠的浪潮，一方面扩展了技术的受众，让更多的人享受到了技术进步带来的福利；另一方面，由于更多的人接触到了新的技术，反过来也提升了技术的实用性和完善程度，加速了技术的成长与发展。

如果说汽车、火车和飞机是从物理上拉近了人与人之间的距离，那么随着互联网的兴起与风靡，世界的距离从逻辑上再一次被拉近了。现今世界的社会结构与人类行为，也已然被互联网深度改造，我们的世界逐渐变得扁平化与碎片化，节奏也越来越快。

根据一项调查，互联网用户通常都没有耐心。例如47%的消费者希望在2秒或更短的时间内完成网页加载，40%的人放弃了加载时间超过3秒的网站，而页面响应时间每延迟1秒就可以使转换率降低7%。实时应答、简单易用，已经是现代互联网系统的必备素质。

SaaS模式的兴起，为传统企业软件系统的商业模式带来了新的思路，这是一次新的技术普惠。一方面，SaaS模式将之前只服务于中大型企业的软件系统放到了互联网，扩展了它的受众；另一方面，由于互联网用户的基本特征和软件诉求，又倒逼了这些软件系统在方方面面进行革新与升级。

技术普惠，导致现代BI系统在设计思路上发生了天翻地覆的变化。

首先，它变得“很轻”，不再需要强制捆绑于企业数据仓库这样的庞然大物之上，就算只根据简单的Excel文件也能进行数据分析。

其次，它的受众变得更加多元化，几乎人人都可以成为数据分析师。现代BI系统不需要IT人员的深度参与，用户直接通过自助的形式，通过简单拖拽、搜索就能得到自己想要的分析结果。

最后，由于经过互联网化的洗礼，即便现代BI系统仍然私有化地部署在企业内部，只服务于企业客户，但它也必须具有快速应答、简单易用的使用体验。从某种角度来看，经过SaaS化这波技术普惠的洗礼，互联网帮助传统企业系统在易用性和用户体验上进行了革命性提升。

如果说SaaS化这波技术普惠为现代BI系统带来了新的思路与契机，那么背后的技术创新则保障了其思想的落地。在传统BI系统的体系中，背后是传统的关系型数据库技术（OLTP数据库）。为了能够解决海量数据下分析查询的性能问题，人们绞尽脑汁，在数据仓库的基础上衍生出众多概念，例如：对数据进行分层，通过层层递进形成数据集市，从而减少最终查询的数据体量；提出数据立方体的概念，通过对数据进行预先处理，以空间换时间，提升查询性能。然而无论如何努力，设计的局限始终是无法突破的瓶颈。OLTP技术由诞生的那一刻起就注定不是为数据分析而生的，于是很多人将目光投向了新的方向。

2003年起，Google陆续发表的三篇论文开启了大数据的技术普惠，Hadoop生态由此开始一发不可收拾，数据分析开启了新纪元。从某种角度来看，以使用Hadoop生态为代表的这类非传统关系型数据库技术所实现的BI系统，可以称为现代BI系统。换装了大马力发动机的现代BI系统在面对海量数据分析的场景时，显得更加游刃有余。然而Hadoop技术也不是银弹，在现代BI系统的构建中仍然面临诸多挑战。在海量数据下要实现多维分析的实时应答，仍旧困难重重。（现代BI系统的典型应用场景是多维分析，某些时候可以直接使用OLAP指代这类场景。）

## 13 OLAP常见架构分类

OLAP领域技术发展至今方兴未艾，分析型数据库百花齐放。然而，看似手上拥有很多筹码的架构师们，有时候却面临无牌可打的窘境，这又是为何呢？为了回答这个问题，我们需要重温一下什么是OLAP，以及实现OLAP的几种常见思路。

之前说过，OLAP名为联机分析，又可以称为多维分析，是由关系型数据库之父埃德加·科德（Edgar Frank Codd）于1993年提出的概念。顾名思义，它指的是通过多种不同的维度审视数据，进行深层次分析。维度可以看作观察数据的一种视角，例如人类能看到的世界是三维的，它包含长、宽、高三个维度。直接一点理解，维度就好比是一张数据表的字段，而多维分析则是基于这些字段进行聚合查询。

那么多维分析通常都包含哪些基本操作呢？为了更好地理解多维分析的概念，可以使用一个立方体的图像具象化操作，如图1-1所示，对于一张销售明细表，数据立方体可以进行如下操作。

![多维分析的常见操作](images/%E5%A4%9A%E7%BB%B4%E5%88%86%E6%9E%90%E7%9A%84%E5%B8%B8%E8%A7%81%E6%93%8D%E4%BD%9C.png)

图1-1　多维分析的常见操作

- 下钻：从高层次向低层次明细数据穿透。例如从“省”下钻到“市”，从“湖北省”穿透到“武汉”和“宜昌”。
- 上卷：和下钻相反，从低层次向高层次汇聚。例如从“市”汇聚成“省”，将“武汉”“宜昌”汇聚成“湖北”。
- 切片：观察立方体的一层，将一个或多个维度设为单个固定值，然后观察剩余的维度，例如将商品维度固定为“足球”。
- 切块：与切片类似，只是将单个固定值变成多个值。例如将商品维度固定成“足球”“篮球”和“乒乓球”。
- 旋转：旋转立方体的一面，如果要将数据映射到一张二维表，那么就要进行旋转，这就等同于行列置换。

为了实现上述这些操作，将常见的OLAP架构大致分成三类。

第一类架构称为ROLAP（Relational OLAP，关系型OLAP）。顾名思义，它直接使用关系模型构建，数据模型常使用星型模型或者雪花模型。这是最先能够想到，也是最为直接的实现方法。因为OLAP概念在最初提出的时候，就是建立在关系型数据库之上的。多维分析的操作，可以直接转换成SQL查询。例如，通过上卷操作查看省份的销售额，就可以转换成类似下面的SQL语句：

```sql
SELECT SUM（价格）FROM 销售数据表 GROUP BY 省
```

但是这种架构对数据的实时处理能力要求很高。试想一下，如果对一张存有上亿行数据的数据表同时执行数十个字段的GROUP BY查询，将会发生什么事情？

第二类架构称为MOLAP（Multidimensional OLAP，多维型OLAP）。它的出现是为了缓解ROLAP性能问题。MOLAP使用多维数组的形式保存数据，其核心思想是借助预先聚合结果，使用空间换取时间的形式最终提升查询性能。也就是说，用更多的存储空间换得查询时间的减少。其具体的实现方式是依托立方体模型的概念。首先，对需要分析的数据进行建模，框定需要分析的维度字段；然后，通过预处理的形式，对各种维度进行组合并事先聚合；最后，将聚合结果以某种索引或者缓存的形式保存起来（通常只保留聚合后的结果，不存储明细数据）。这样一来，在随后的查询过程中，就可以直接利用结果返回数据。但是这种架构也并不完美。维度预处理可能会导致数据的膨胀。这里可以做一次简单的计算，以图1-1中所示的销售明细表为例。如果数据立方体包含了5个维度（字段），那么维度组合的方式则有25 （2n ，n=维度个数）个。例如，省和市两个维度的组合就有<湖北，武汉>、<湖北、宜昌>、<武汉、湖北>、<宜昌、湖北>等。可想而知，当维度数据基数较高的时候，（高基数意味着重复相同的数据少。）其立方体预聚合后的数据量可能会达到10到20倍的膨胀。一张千万级别的数据表，就有可能膨胀到亿级别的体量。人们意识到这个问题之后，虽然也实现了一些能够降低膨胀率的优化手段，但并不能完全避免。另外，由于使用了预处理的形式，数据立方体会有一定的滞后性，不能实时进行数据分析。而且，立方体只保留了聚合后的结果数据，导致无法查询明细数据。

第三类架构称为HOLAP（Hybrid OLAP，混合架构的OLAP）。这种思路可以理解成ROLAP和MOLAP两者的集成。这里不再展开，我们重点关注ROLAP和MOLAP。

## 1.4 OLAP实现技术的演进

在介绍了OLAP几种主要的架构之后，再来看看它们背后技术的演进过程。我把这个演进过程简单划分成两个阶段。

第一个可以称为传统关系型数据库阶段。在这个阶段中，OLAP主要基于以Oracle、MySQL为代表的一众关系型数据实现。在ROLAP架构下，直接使用这些数据库作为存储与计算的载体；在MOLAP架构下，则借助物化视图的形式实现数据立方体。在这个时期，不论是ROLAP还是MOLAP，在数据体量大、维度数目多的情况下都存在严重的性能问题，甚至存在根本查询不出结果的情况。

第二个可以称为大数据技术阶段。由于大数据处理技术的普及，人们开始使用大数据技术重构ROLAP和MOLAP。以ROLAP架构为例，传统关系型数据库就被Hive和SparkSQL这类新兴技术所取代。虽然，以Spark为代表的分布式计算系统，相比Oracle这类传统数据库而言，在面向海量数据的处理性能方面已经优秀很多，但是直接把它们作为面向终端用户的在线查询系统还是太慢了。我们的用户普遍缺乏耐心，如果一个查询响应需要几十秒甚至数分钟才能返回，那么这套方案就完全行不通。再看MOLAP架构，MOLAP背后也转为依托MapReduce或Spark这类新兴技术，将其作为立方体的计算引擎，加速立方体的构建过程。其预聚合结果的存储载体也转向HBase这类高性能分布式数据库。大数据技术阶段，主流MOLAP架构已经能够在亿万级数据的体量下，实现毫秒级的查询响应时间。尽管如此，MOLAP架构依然存在维度爆炸、数据同步实时性不高的问题。

不难发现，虽然OLAP在经历了大数据技术的洗礼之后，其各方面性能已经有了脱胎换骨式的改观，但不论是ROLAP还是MOLAP，仍然存在各自的痛点。

如果单纯从模型角度考虑，很明显ROLAP架构更胜一筹。因为关系模型拥有最好的“群众基础”，也更简单且容易理解。它直接面向明细数据查询，由于不需要预处理，也就自然没有预处理带来的负面影响（维度组合爆炸、数据实时性、更新问题）。那是否存在这样一种技术，它既使用ROLAP模型，同时又拥有比肩MOLAP的性能呢？

## 1.5 一匹横空出世的黑马

我从2012年正式进入大数据领域，开始从事大数据平台相关的基础研发工作。2016年我所在的公司启动了战略性创新产品的规划工作，自此我开始将工作重心转到设计并研发一款具备现代化SaaS属性的BI分析类产品上。为了实现人人都是分析师的最终目标，这款BI产品必须至少具备如下特征。

- 一站式：下至数百条数据的个人Excel表格，上至数亿级别的企业数据，都能够在系统内部被直接处理。
- 自服务，简单易用：面向普通用户而非专业IT人员，通过简单拖拽或搜索维度，就能完成初步的分析查询。分析内容可以是自定义的，并不需要预先固定好。
- 实时应答：无论数据是什么体量级别，查询必须在毫秒至1秒内返回。数据分析是一个通过不断提出假设并验证假设的过程，只有做到快速应答，这种分析过程的路径才算正确。
- 专业化、智能化：需要具备专业化程度并具备智能化的提升空间，需要提供专业的数学方法。

为了满足上述产品特性，我们在进行底层数据库技术选型的时候可谓是绞尽脑汁。上文曾提及，以Spark为代表的新一代ROLAP方案虽然可以一站式处理海量数据，但无法真正做到实时应答和高并发，它更适合作为一个后端的查询系统。而新一代的MOLAP方案虽然解决了大部分查询性能的瓶颈问题，能够做到实时应答，但数据膨胀和预处理等问题依然没有被很好解决。除了上述两类方案之外，也有一种另辟蹊径的选择，即摒弃ROLAP和MOALP转而使用搜索引擎来实现OLAP查询，ElasticSearch是这类方案的代表。ElasticSearch支持实时更新，在百万级别数据的场景下可以做到实时聚合查询，但是随着数据体量的继续增大，它的查询性能也将捉襟见肘。

难道真的是鱼与熊掌不可兼得了吗？直到有一天，在查阅一份Spark性能报告的时候，我不经意间看到了一篇性能对比的博文。Spark的对手是一个我从来没有见过的陌生名字，在10亿条测试数据的体量下，Spark这个我心目中的绝对王者，居然被对手打得落花流水，查询响应时间竟然比对手慢数90%之多。而对手居然只使用了一台配有i5 CPU、16GB内存和SSD磁盘的普通PC电脑。我揉了揉眼睛，定了定神，这不是做梦。ClickHouse就这样进入了我的视野。

### 1.5.1 天下武功唯快不破

我对ClickHouse的最初印象极为深刻，其具有ROLAP、在线实时查询、完整的DBMS、列式存储、不需要任何数据预处理、支持批量更新、拥有非常完善的SQL支持和函数、支持高可用、不依赖Hadoop复杂生态、开箱即用等许多特点。特别是它那夸张的查询性能，我想大多数刚接触ClickHouse的人也一定会因为它的性能指标而动容。在一系列官方公布的基准测试对比中，ClickHouse都遥遥领先对手，这其中不乏一些我们耳熟能详的名字。

所有用于对比的数据库都使用了相同配置的服务器，在单个节点的情况下，对一张拥有133个字段的数据表分别在1000万、1亿和10亿三种数据体量下执行基准测试，基准测试的范围涵盖43项SQL查询。在1亿数据集体量的情况下，ClickHouse的平均响应速度是Vertica的2.63倍、InfiniDB的17倍、MonetDB的27倍、Hive的126倍、MySQL的429倍以及Greenplum的10倍。详细的测试结果可以查阅https://clickhouse.yandex/benchmark.html 。

### 1.5.2 社区活跃

ClickHouse是一款开源软件，遵循Apache License 2.0协议，所以它可以被免费使用。同时它的开源社区也非常跃度，截至本书完稿时，其在全球范围内拥有接近400位贡献者。从本书动笔至初稿完成，ClickHouse已经完成了10多个版本的发布，并不是我写得慢，而是它的版本发布频率确实惊人，基本保持着每个月发布一次版本的更新频率。友好的开源协议、活跃的社区加上积极的响应，意味着我们可以及时获取最新特性并得到修复缺陷的补丁。

## 1.6 ClickHouse的发展历程

ClickHouse背后的研发团队是来自俄罗斯的Yandex公司。这是一家俄罗斯本土的互联网企业，于2011年在纳斯达克上市，它的核心产品是搜索引擎。根据最新的数据显示，Yandex占据了本国47%以上的搜索市场，是现今世界上最大的俄语搜索引擎。Google是它的直接竞争对手。

众所周知，在线搜索引擎的营收来源非常依赖流量和在线广告业务。所以，通常搜索引擎公司为了更好地帮助自身及用户分析网络流量，都会推出自家的在线流量分析产品，例如Google的GoogleAnalytics、百度的百度统计。Yandex也不例外，Yandex.Metrica就是这样一款用于在线流量分析的产品（https://metrica.yandex.com）。

ClickHouse就是在这样的产品背景下诞生的，伴随着Yandex.Metrica业务的发展，其底层架构历经四个阶段，一步一步最终形成了大家现在所看到的ClickHouse。纵观这四个阶段的发展，俨然是数据分析产品形态以及OLAP架构历史演进的缩影。通过了解这段演进过程，我们能够更透彻地了解OLAP面对的挑战，以及ClickHouse能够解决的问题。

### 1.6.1 顺理成章的MySQL时期

作为一款在线流量分析产品，对其功能的要求自然是分析流量了。早期的Yandex.Metrica以提供固定报表的形式帮助用户进行分析，例如分析访问者使用的设备、访问者来源的分布之类。其实这也是早期分析类产品的典型特征之一，分析维度和场景是固定的，新的分析需求往往需要IT人员参与。

从技术角度来看，当时还处于关系型数据库称霸的时期，所以Yandex在内部其他产品中使用了MySQL数据库作为统计信息系统的底层存储软件。Yandex.Metrica的第一版架构顺理成章延续了这套内部稳定成熟的MySQL方案，并将其作为它的数据存储和分析引擎的解决方案。

因为Yandex内部的这套MySQL方案使用了MyISAM表引擎，所以Yandex.Metrica也延续了表引擎的选择。这类分析场景更关注数据写入和查询的性能，不关心事务操作（MyISAM表引擎不支持事务特性）。相比InnoDB表引擎，MyISAM表引擎在分析场景中具有更好的性能。

业内有一个常识性的认知，按顺序存储的数据会拥有更高的查询性能。因为读取顺序文件会用更少的磁盘寻道和旋转延迟时间（这里主要指机械磁盘），同时顺序读取也能利用操作系统层面文件缓存的预读功能，所以数据库的查询性能与数据在物理磁盘上的存储顺序息息相关。然而这套MySQL方案无法做到顺序存储。

MyISAM表引擎使用B+树结构存储索引，而数据则使用另外单独的存储文件（InnoDB表引擎使用B+树同时存储索引和数据，数据直接挂载在叶子节点中）。如果只考虑单线程的写入场景，并且在写入过程中不涉及数据删除或者更新操作，那么数据会依次按照写入的顺序被写入文件并落至磁盘。然而现实的场景不可能如此简单。

流量的数据采集链路是这样的：网站端的应用程序首先通过Yandex提供的站点SDK实时采集数据并发送到远端的接收系统，再由接收系统将数据写入MySQL集群。整个过程都是实时进行的，并且数据接收系统是一个分布式系统，所以它们会并行、随机将数据写入MySQL集群。这最终导致了数据在磁盘中是完全随机存储的，并且会产生大量的磁盘碎片。

市面上一块典型的7200转SATA磁盘的IOPS（每秒能处理的请求数）仅为100左右，也就是说每秒只能执行100次随机读取。假设一次随机读取返回10行数据，那么查询100000行记录则需要至少100秒，这种响应时间显然是不可接受的。

RAID可以提高磁盘IOPS性能，但并不能解决根本问题。SSD随机读取性能很高，但是考虑到硬件成本和集群规模，不可能全部采取SSD存储。

随着时间的推移，MySQL中的数据越来越多（截至2011年，存储的数据超过5800亿行）。虽然Yandex又额外做了许多优化，成功地将90%的分析报告控制在26秒内返回，但是这套技术方案越来越显得力不从心。

### 1.6.2 另辟蹊径的Metrage时期

由于MySQL带来的局限性，Yandex自研了一套全新的系统并命名为Metrage。Metrage在设计上与MySQL完全不同，它选择了另外一条截然不同的道路。首先，在数据模型层面，它使用Key-Value模型（键值对）代替了关系模型；其次，在索引层面，它使用LSM树代替了B+树；最后，在数据处理层面，由实时查询的方式改为了预处理的方式。

LSM树也是一种非常流行的索引结构，发源于Google的BigTable，现在最具代表性的使用LSM树索引结构的系统是HBase。LSM本质上可以看作将原本的一棵大树拆成了许多棵小树，每一批次写入的数据都会经历如下过程。首先，会在内存中构建出一棵小树，构建完毕即算写入成功（这里会通过预写日志的形式，防止因内存故障而导致的数据丢失）。写入动作只发生在内存中，不涉及磁盘操作，所以极大地提升了数据写入性能。其次，小树在构建的过程中会进行排序，这样就保证了数据的有序性。最后，当内存中小树的数量达到某个阈值时，就会借助后台线程将小树刷入磁盘并生成一个小的数据段。在每个数据段中，数据局部有序。也正因为数据有序，所以能够进一步使用稀疏索引来优化查询性能。借助LSM树索引，可使得Metrage引擎在软硬件层面同时得到优化（磁盘顺序读取、预读缓存、稀疏索引等），最终有效提高系统的综合性能。

如果仅拥有索引结构的优化，还不足以从根本上解决性能问题。Metrage设计的第二个重大转变是通过预处理的方式，将需要分析的数据预先聚合。这种做法类似数据立方体的思想，首先对分析的具体场景实施立方体建模，框定所需的维度和度量以形成数据立方体；接着预先计算立方体内的所有维度组合；最后将聚合的结果数据按照Key-Value的形式存储。这样一来，对于固定分析场景，就可以直接利用数据立方体的聚合结果立即返回相关数据。这套系统的实现思路和现今的一些MOLAP系统如出一辙。

通过上述一系列的转变，Metrage为Yandex.Metrica的性能带来了革命性提升。截至2015年，在Metrage内存储了超过3万亿行的数据，其集群规模超过了60台服务器，查询性能也由先前的26秒降低到了惊人的1秒以内。然而，使用立方体这类预先聚合的思路会带来一个新的问题，那就是维度组合爆炸，因为需要预先对所有的维度组合进行计算。那么维度组合的方式具体有多少种呢？它的计算公式是2N （N=维度数量）。可以做一次简单的计算，例如5个维度的组合方式会有25=32种，而9个维度的组合方式则会多达29 =512种，这是一种指数级的增长方式。维度组合的爆炸会直接导致数据膨胀，有时候这种膨胀可能会多达10～20倍。

### 1.6.3 自我突破的OLAPServer时期

如果说Metrage系统是弥补Yandex.Metrica性能瓶颈的产物，那么OLAPServer系统的诞生，则是产品形态升级倒逼的结果。在Yandex.Metrica的产品初期，它只支持固定报表的分析功能。随着时间的推移，这种固定化的分析形式早已不能满足用户的诉求，于是Yandex.Metrica计划推出自定义分析报告的功能。然而Metrage系统却无法满足这类自定义的分析场景，因为它需要预先聚合，并且只提供了内置的40多种固定分析场景。单独为每一个用户提供面向个人的预聚合功能显然是不切实际的。在这种背景下，Yandex.Metrica的研发团队只有寻求自我突破，于是自主研发了OLAPServer系统。

OLAPServer系统被设计成专门处理自定义报告这类临时性分析需求的系统，与Metrage系统形成互补的关系。结合之前两个阶段的建设经验，OLAPServer在设计思路上可以说是取众家之长。在数据模型方面，它又换回了关系模型，因为相比Key-Value模型，关系模型拥有更好的描述能力。使用SQL作为查询语言，也将会拥有更好的“群众基础”。而在存储结构和索引方面，它结合了MyISAM和LSM树最精华的部分。在存储结构上，它与MyISAM表引擎类似，分为了索引文件和数据文件两个部分。在索引方面，它并没有完全沿用LSM树，而是使用了LSM树所使用到的稀疏索引。在数据文件的设计上，则沿用了LSM树中数据段的思想，即数据段内数据有序，借助稀疏索引定位数据段。在有了上述基础之后，OLAPServer又进一步引入了列式存储的思想，将索引文件和数据文件按照列字段的粒度进行了拆分，每个列字段各自独立存储，以此进一步减少数据读取的范围。

虽然OLAPServer在实时聚合方面的性能相比MySQL有了质的飞跃，但从功能的完备性角度来看，OLAPServer还是差了一个量级。如果说MySQL可以称为数据库管理系统（DBMS），那么OLAPServer只能称为数据库。因为OLAPServer的定位只是和Metrage形成互补，所以它缺失了一些基本的功能。例如，它只有一种数据类型，即固定长度的数值类型，且没有DBMS应有的基本管理功能（DDL查询等）。

### 1.6.4 水到渠成的ClickHouse时代

现在，一个新的选择题摆在了Yandex.Metrica研发团队的面前，实时聚合还是预先聚合？预先聚合方案在查询性能方面带来了质的提升，成功地将之前的报告查询时间从26秒降低到了1秒以内，但同时它也带来了新的难题。

1. 由于预先聚合只能支持固定的分析场景，所以它无法满足自定义分析的需求。
2. 维度组合爆炸会导致数据膨胀，这样会造成不必要的计算和存储开销。因为用户并不一定会用到所有维度的组合，那些没有被用到的组合将会成为浪费的开销。
3. 流量数据是在线实时接收的，所以预聚合还需要考虑如何及时更新数据。

经过这么一分析，预先聚合的方案看起来似乎也没有那么完美。这是否表示实时聚合的方案更优呢？实时聚合方案意味着一切查询都是动态、实时的，从用户发起查询的那一刻起，整个过程需要在一秒内完成并返回，而在这个查询过程的背后，可能会涉及数亿行数据的处理。如果做不到这么快的响应速度，那么这套方案就不可行，因为用户都讨厌等待。很显然，如果查询性可以得到保障，实时聚合会是一个更为简洁的架构。由于OLAPServer的成功使用经验，选择倾向于实时聚合这一方。

OLAPServer在查询性能方面并不比Metrage差太多，在查询的灵活性方面反而更胜一筹。于是Yandex.Metrica研发团队以OLAPServer为基础进一步完善，以实现一个完备的数据库管理系统（DBMS）为目标，最终打造出了ClickHouse，并于2016年开源。纵览Yandex.Metrica背后技术的发展历程，ClickHouse的出现似乎是一个水到渠成的结果。

ClickHouse的发展历程如表1-1所示。

表1-1　ClickHouse的发展历程

![ClickHouse的发展历程](images/ClickHouse%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E7%A8%8B.png)

## 1.7 ClickHouse的名称含义

经过上一节的介绍，大家知道了ClickHouse由雏形发展至今一共经历了四个阶段。它的初始设计目标是服务自己公司的一款名叫Yandex.Metrica的产品。Metrica是一款Web流量分析工具，基于前方探针采集行为数据，然后进行一系列的数据分析，类似数据仓库的OLAP分析。而在采集数据的过程中，一次页面click（点击），会产生一个event（事件）。至此，整个系统的逻辑就十分清晰了，那就是基于页面的点击事件流，面向数据仓库进行OLAP分析。所以ClickHouse的全称是Click Stream，Data WareHouse，简称ClickHouse，如图1-2所示。

图1-2　ClickHouse名称缩写的含义

![ClickHouse名称缩写的含义](images/ClickHouse%E5%90%8D%E7%A7%B0%E7%BC%A9%E5%86%99%E7%9A%84%E5%90%AB%E4%B9%89.png)

## 1.8 ClickHouse适用的场景

因为ClickHouse在诞生之初是为了服务Yandex自家的Web流量分析产品Yandex.Metrica，所以在存储数据超过20万亿行的情况下，ClickHouse做到了90%的查询都能够在1秒内返回的惊人之举。随后，ClickHouse进一步被应用到Yandex内部大大小小数十个其他的分析场景中。可以说ClickHouse具备了人们对一款高性能OLAP数据库的美好向往，所以它基本能够胜任各种数据分析类的场景，并且随着数据体量的增大，它的优势也会变得越为明显。

ClickHouse非常适用于商业智能领域（也就是我们所说的BI领域），除此之外，它也能够被广泛应用于广告流量、Web、App流量、电信、金融、电子商务、信息安全、网络游戏、物联网等众多其他领域。

## 1.9 ClickHouse不适用的场景

ClickHouse作为一款高性能OLAP数据库，虽然足够优秀，但也不是万能的。我们不应该把它用于任何OLTP事务性操作的场景，因为它有以下几点不足。

- 不支持事务。
- 不擅长根据主键按行粒度进行查询（虽然支持），故不应该把ClickHouse当作Key-Value数据库使用。
- 不擅长按行删除数据（虽然支持）。

这些弱点并不能视为ClickHouse的缺点，事实上其他同类高性能的OLAP数据库同样也不擅长上述的这些方面。因为对于一款OLAP数据库而言，上述这些能力并不是重点，只能说这是为了极致查询性能所做的权衡。

## 1.10 有谁在使用ClickHouse

除了Yandex自己以外，ClickHouse还被众多商业公司或研究组织成功地运用到了它们的生产环境。欧洲核子研究中心（CERN）将它用于保存强对撞机试验后记录下的数十亿事件的测量数据，并成功将先前查找数据的时间由几个小时缩短到几秒。著名的CDN服务厂商CloudFlare将ClickHouse用于HTTP的流量分析。国内的头条、阿里、腾讯和新浪等一众互联网公司对ClickHouse也都有涉猎。更多详情可以参考ClickHouse官网的案例介绍（https://clickhouse.yandex/）。

## 1.11 本章小结

本章开宗明义，介绍了作为一线从业者的我在经历BI系统从传统转向现代的过程中的所思所想，以及如何在机缘巧合之下发现了令人印象深刻的ClickHouse。本章抽丝剥茧，揭开了ClickHouse诞生的缘由。原来ClickHouse背后的研发团队是来自俄罗斯的Yandex公司，Yandex为了支撑Web流量分析产品Yandex.Metrica，在历经MySQL、Metrage和OLAPServer三种架构之后，集众家之所长，打造出了ClickHouse。在下一章，我们将进一步详细介绍ClickHouse的架构，对它的核心特点进行深入探讨。